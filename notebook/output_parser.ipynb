{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output parse\n",
    "\n",
    "docs: https://python.langchain.com/en/latest/modules/prompts/output_parsers/getting_started.html\n",
    "\n",
    "对输出做约定，默认输出的是文本，如果希望是结构化的数据，可以用 `langchain.output_parses` 来实现，包括：\n",
    "\n",
    "1. 自定义输出（比如一个类对象）\n",
    "2. 输出JSON\n",
    "3. 输出List\n",
    "4. 输出枚举\n",
    "\n",
    "本质：\n",
    "\n",
    "```python\n",
    "# 1.选择或者自定义output parse\n",
    "parse = xxx # langchain.output_parses里有很多\n",
    "# 2.获得格式说明\n",
    "out_fmt = parse.get_format_instructions()\n",
    "# 3.作为prompt 部分变量\n",
    "prompt = PromptTemplate(\n",
    "    template='Answer the user query.\\n{format_instructions}\\n{query}\\n',\n",
    "    input_variables=['query'],\n",
    "    partial_variables={'format_instructions': out_fmt} \n",
    ")\n",
    "# 4.parse 解析llm返回的结果字符串\n",
    "output: str = llm(prompt.format_prompt(k,v ... xxx).to_string())\n",
    "output_parse.parse(output)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get more structured information from the output of the model\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List, Optional, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init OpenAI env\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'openai' from '/Users/lingoace/Documents/GitHub/llm-101/venv/lib/python3.10/site-packages/openai/__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from app.init import init_llm\n",
    "init_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'text-davinci-003'\n",
    "temperature = 0.0\n",
    "llm = OpenAI(model_name=model_name, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description='question to set up a joke')\n",
    "    punchline: str = Field(description='answer to resolve the joke')\n",
    "\n",
    "    @validator('setup')\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != '?':\n",
    "            raise ValueError('question must end with a question mark')\n",
    "        return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# set up a parse, inject instructions into the prompt template\n",
    "parse = PydanticOutputParser(pydantic_object=Joke)\n",
    "print(parse.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], output_parser=None, partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n```'}, template='Answer the user query.\\n{format_instructions}\\n{query}\\n', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template='Answer the user query.\\n{format_instructions}\\n{query}\\n',\n",
    "    input_variables=['query'],\n",
    "    partial_variables={'format_instructions': parse.get_format_instructions()} \n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\"setup\": \"What do you call a pig that does karate?\", \"punchline\": \"A pork chop!\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Joke(setup='What do you call a pig that does karate?', punchline='A pork chop!')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm(prompt.format_prompt(query='What do you call a pig that does karate?').to_string())\n",
    "print(output)\n",
    "\n",
    "# structed output\n",
    "parse.parse(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CommaSeparatedListOutputParser\n",
    "docs: https://python.langchain.com/en/latest/modules/prompts/output_parsers/examples/comma_separated.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Java, C++, Python, JavaScript, Ruby\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Java', 'C++', 'Python', 'JavaScript', 'Ruby']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parse = CommaSeparatedListOutputParser() # List\n",
    "\n",
    "format_instructions = output_parse.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template = 'List five {subject}.\\n{format_instructions}',\n",
    "    input_variables=['subject'],\n",
    "    partial_variables={'format_instructions': format_instructions}\n",
    ")\n",
    "\n",
    "output = llm(prompt.format_prompt(subject='programming languages').to_string())\n",
    "print(output)\n",
    "\n",
    "output_parse.parse(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured\n",
    "docs: https://python.langchain.com/en/latest/modules/prompts/output_parsers/examples/structured.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredOutputParser(response_schemas=[ResponseSchema(name='answer', description='answer to the question', type='string'), ResponseSchema(name='source', description=\"source used to answer the user's question, should be a website.\", type='string')])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# here we define the response schema we want to receive.\n",
    "response_schemas = [\n",
    "    ResponseSchema(name='answer', description='answer to the question'),\n",
    "    ResponseSchema(name='source', description=\"source used to answer the user's question, should be a website.\")\n",
    "]\n",
    "output_parse = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "output_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"answer\": string  // answer to the question\\n\\t\"source\": string  // source used to answer the user\\'s question, should be a website.\\n}\\n```'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_instructions = output_parse.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"answer\": \"Paris\",\n",
      "\t\"source\": \"https://en.wikipedia.org/wiki/Paris\"\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'Paris', 'source': 'https://en.wikipedia.org/wiki/Paris'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"answer the users question as best as possible.\\n{format_instructions}\\n{question}\",\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "output = llm(prompt.format_prompt(question=\"what's the capital of france?\").to_string())\n",
    "print(output)\n",
    "\n",
    "# structed output\n",
    "output_parse.parse(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Paris', 'source': 'https://en.wikipedia.org/wiki/Paris'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also for chat_model\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0)\n",
    "chat_prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template('answer the users question as best as possible.\\n{format_instructions}\\n{question}'),\n",
    "    ],\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "_input = chat_prompt.format_prompt(question=\"what's the capital of france?\")\n",
    "output = chat_model(_input.to_messages())\n",
    "output_parse.parse(output.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
