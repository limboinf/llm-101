{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.Chain异步API, 所有 以a开头的方法都是异步的，底层asyncio\n",
    "# 当前LLMChain(其方法 arun, apredict, acall) 支持异步\n",
    "# ChatVectorDBChain, and QA chains 也支持，其他的还在路上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adjective': 'corny', 'text': 'Why did the tomato turn red? Because it saw the salad dressing!'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'Why did the tomato turn red? Because it saw the salad dressing!'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 2. __call__ && run 方法\n",
    "\n",
    "# 所有继承Chain都有 __call__方法\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "prompt_template = \"Tell me a {adjective} joke\"\n",
    "llm_chain = LLMChain(llm=chat, prompt=PromptTemplate.from_template(prompt_template))\n",
    "\n",
    "print(llm_chain(inputs={\"adjective\": \"corny\"}))\n",
    "\n",
    "# 默认情况下，__call__ 返回输入键值和输出键值。可以将 return_only_outputs 设置为 True，以仅返回输出键值。\n",
    "\n",
    "llm_chain(\"corny\", return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Why did the tomato turn red? Because it saw the salad dressing!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm_chain only has one output key, so we can use run\n",
    "print(llm_chain.output_keys)\n",
    "# 如果链只输出一个输出键（即其output_keys中只有一个元素），则可以使用run方法。\n",
    "# 请注意，run方法输出的是字符串而不是字典。\n",
    "llm_chain.run({\"adjective\": \"corny\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the tomato turn red? Because it saw the salad dressing!\n",
      "Why did the tomato turn red? Because it saw the salad dressing!\n",
      "{'adjective': 'corny', 'text': 'Why did the tomato turn red? Because it saw the salad dressing!'}\n",
      "{'adjective': 'corny', 'text': 'Why did the tomato turn red? Because it saw the salad dressing!'}\n"
     ]
    }
   ],
   "source": [
    "# 如果只有一个输入键，则可以直接输入字符串，无需指定输入映射。\n",
    "\n",
    "# These two are equivalent\n",
    "print(llm_chain.run({\"adjective\": \"corny\"}))\n",
    "print(llm_chain.run(\"corny\"))\n",
    "\n",
    "# These two are also equivalent\n",
    "print(llm_chain(\"corny\"))\n",
    "print(llm_chain({\"adjective\": \"corny\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.custom chain\n",
    "# 需要继承Chain并实现下面方法\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from pydantic import Extra\n",
    "\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForChainRun,\n",
    "    CallbackManagerForChainRun,\n",
    ")\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.prompts.base import BasePromptTemplate\n",
    "\n",
    "\n",
    "class MyCustomChain(Chain):\n",
    "    \"\"\"\n",
    "    An example of a custom chain.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt: BasePromptTemplate\n",
    "    llm: BaseLanguageModel\n",
    "    output_key: str = \"text\"  #: :meta private:\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"这个pydantic对象的配置.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid            # private\n",
    "        arbitrary_types_allowed = True  # 允许任意的类型\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"将会是提示所期望的任何键。.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return self.prompt.input_variables\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"将始终返回文本键。.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return [self.output_key]\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        # 自定义链逻辑放在这里。\n",
    "        \n",
    "        # 这只是一个模仿LLMChain的示例\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "        print(f'自定义chain: prompt_value={prompt_value}')\n",
    "\n",
    "        # 每当调用一个语言模型或另一个链时，应该将回调管理器传递给它。\n",
    "        # 这样可以通过在外部运行时注册的任何回调来跟踪内部运行。\n",
    "        # 可以通过调用`run_manager.get_child()`来获取此回调管理器，如下所示。\n",
    "        response = self.llm.generate_prompt(\n",
    "            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n",
    "        )\n",
    "        \n",
    "        print(f'自定义chain: response={response}')\n",
    "\n",
    "        # 如果你想记录关于这次运行的一些信息，你可以通过调用`run_manager`上的方法来实现，\n",
    "        # 如下所示。这将触发为该事件注册的任何回调函数。\n",
    "        if run_manager:\n",
    "            run_manager.on_text(\"Log something about this run\")\n",
    "\n",
    "        return {self.output_key: response.generations[0][0].text}\n",
    "\n",
    "    async def _acall(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        # 同上，只不过异步\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "\n",
    "        response = await self.llm.agenerate_prompt(\n",
    "            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n",
    "        )\n",
    "        \n",
    "        # response 格式：\n",
    "        # generations=[[ChatGeneration(\n",
    "        #       text='response xxx', \n",
    "        #       generation_info=None, \n",
    "        #       message=AIMessage(content='response xxx', additional_kwargs={}, example=False))]]\n",
    "        # llm_output={'token_usage': {'prompt_tokens': 14, 'completion_tokens': 15, 'total_tokens': 29}, 'model_name': 'gpt-3.5-turbo'} \n",
    "        # run=RunInfo(run_id=UUID('224a4b3b-89c3-43fa-816e-3c78441674e5'))\n",
    "\n",
    "        if run_manager:\n",
    "            await run_manager.on_text(\"Log something about this run\")\n",
    "\n",
    "        return {self.output_key: response.generations[0][0].text}\n",
    "\n",
    "    @property\n",
    "    def _chain_type(self) -> str:\n",
    "        return \"my_custom_chain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "自定义chain: prompt_value=text='tell us a joke about callbacks'\n",
      "自定义chain: response=generations=[[ChatGeneration(text='Why did the callback function feel lonely? Because it never gets called back!', generation_info=None, message=AIMessage(content='Why did the callback function feel lonely? Because it never gets called back!', additional_kwargs={}, example=False))]] llm_output={'token_usage': {'prompt_tokens': 14, 'completion_tokens': 15, 'total_tokens': 29}, 'model_name': 'gpt-3.5-turbo'} run=RunInfo(run_id=UUID('224a4b3b-89c3-43fa-816e-3c78441674e5'))\n",
      "Log something about this run\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Why did the callback function feel lonely? Because it never gets called back!'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks.stdout import StdOutCallbackHandler\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "\n",
    "chain = MyCustomChain(\n",
    "    prompt=PromptTemplate.from_template(\"tell us a joke about {topic}\"),\n",
    "    llm=ChatOpenAI(),\n",
    ")\n",
    "\n",
    "# run 最终调用 __call__\n",
    "# 源码 return self(kwargs, callbacks=callbacks, tags=tags)[_output_key]\n",
    "chain.run({\"topic\": \"callbacks\"}, callbacks=[StdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.debugging chains\n",
    "# 将 verbose 设置为 True 将在运行 Chain 对象时打印一些内部状态。\n",
    "# XXXChain(llm=llm or chat, prompt=prompt, memory=xx, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.Loading from LangChainHub\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
